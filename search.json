[
  {
    "objectID": "quizzes.html",
    "href": "quizzes.html",
    "title": "Quizzes",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nDec 30, 2023\n\n\nPrerequsite test\n\n\n \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ES 335 Machine Learning (August 2024)",
    "section": "",
    "text": "Please join the Slack channel for the course.\n\n\n\n\n\n\nSummary\n\n\n\n\nInstructor: Nipun Batra nipun.batra@iitgn.ac.in\n\nOffice: 13/401C\nYouTube video showing the directions to my office\n\nTeaching Assistants:\n\nAyush Shrivastava shrivastavaayush@iitgn.ac.in\n\nCourse Timings:\n\nLectures: Monday, Wednesday 8:30 AM to 10 AM in 10/103\nTutorials: Friday: 1130 to 1 PM in 10/103\n\n\n\n\n\n\n\n\n\n\nPre-requisite exam\n\n\n\nThe pre-requisite exam has to be submitted by 6th August 2024, 9 PM. Details on how to submit the exam are given in the exam itself. The exam is open book and open internet.\n\n\n\n\nPre-requisites:\n\nGood experience in Python programming\nProbability\nLinear Algebra\n\nCourse preparation: Students are encouraged to study some of the following to refresh their understanding of some of the prerequisities before the course formally begins.\n\nFirst four chapters of the Python Data Science handbook\nSome material on Linear Algebra\nKhan academy course on Stats and Probability\n\n\n\nReference textbooks:\n\nGareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani. An Introduction to Statistical Learning with Applications in R\nChristopher Bishop. Pattern Recognition and Machine Learning. Springer, 2006.[Freely available online]\nFriedman J, Hastie T, Tibshirani R. The elements of statistical learning. New York, NY, USA:: Springer series in statistics; 2001.[Freely available online]\nDuda RO, Hart PE, Stork DG. Pattern classification. John Wiley & Sons; 2012 Nov 9.\nMitchell TM. Machine learning. 1997. Burr Ridge, IL: McGraw Hill. 1997;45(37):870-7.\nMurphy, K. Machine Learning: A Probabilistic Perspective. MIT Press\nGoodfellow I, Bengio Y, Courville A, Bengio Y. Deep learning. Cambridge: MIT press; 2016 Nov 18.[Freely available online]\n\n\n\nSome other ML courses\n\nNPTEL course by Balaram Ravindran\nCMU course by Tom Mitchell and Maria-Florina Balcan\nCoursera ML course by Andrew Ng\nFAST.ai course on ML\nPractical deep learning for coders by FAST.ai\nCourse by Alex Ihler, UCI"
  },
  {
    "objectID": "tentative.html",
    "href": "tentative.html",
    "title": "Tentative deadlines",
    "section": "",
    "text": "January 24: First assignment due\nFebruary 2: First quiz\nEarly Feb: Second assignment released\nSometime in February 19-23 as per academic calendar during mid sem exams: Second quiz\nMarch 1: Second assignment due, third assignment released\nMarch 15: Third quiz\nMarch 21: Third assignment due, fourth assignment released\nApril 15: Fourth assignment due\nEnd sem: Fourth quiz"
  },
  {
    "objectID": "exams/prereq.html",
    "href": "exams/prereq.html",
    "title": "Prerequsite test",
    "section": "",
    "text": "Instructions\n\nThis test is open book, open internet, open notes. You can use any resources you want to solve the problems.\nYou should be typing your answers in a Jupyter notebook.\nThe submission would be a link to a public GitHub repository containing the notebook. Fill this form to submit your solution.\nA random subset of students may have a viva post the exam. The viva would be based on the notebook and the solutions you have provided.\nThe test is open till 6th January 2024 9 PM. You can submit your solutions anytime before that.\nThis problem has to be solved individually. You cannot collaborate with anyone else.\nThe code should be written using Python.\nSome questions may require you to answer in text. You can use markdown cells to write your answers. Some questions may require you to write code. You can use code cells to write your code. Some questions may require you to write mathematical expressions. You can use LaTeX to write your expressions. You can write such LaTeX expressions in markdown cells.\nFor any other questions, please ask on the General channel on Slack.\n\n\n\n\nQuestions\n\nHow many multiplications and additions do you need to perform a matrix multiplication between a (n, k) and (k, m) matrix? Explain.\nWrite Python code to multiply the above two matrices. Solve using list of lists and then use numpy. Compare the timing of both solutions. Which one is faster? Why?\nFinding the highest element in a list requires one pass of the array. Finding the second highest element requires 2 passes of the the array. Using this method, what is the time complexity of finding the median of the array? Can you suggest a better method? Can you implement both these methods in Python and compare against numpy.median routine in terms of time?\nWhat is the gradient of the following function with respect to x and y? \\[\nx^2y+y^3\\sin(x)\n\\]\nUse JAX to confirm the gradient evaluated by your method matches the analytical solution corresponding to a few random values of x and y\nUse sympy to confirm that you obtain the same gradient analytically.\nCreate a Python nested dictionary to represent hierarchical information. We want to store record of students and their marks. Something like:\n\n2022\n\nBranch 1\n\nRoll Number: 1, Name: N, Marks:\n\nMaths: 100, English: 70 …\n\n\nBranch 2\n\n2023\n\nBranch 1\nBranch 2\n\n2024\n\nBranch 1\nBranch 2\n\n2025\n\nBranch 1\nBranch 2\n\n\nStore the same information using Python classes. We have an overall database which is a list of year objects. Each year contains a list of branches. Each branch contains a list of students. Each student has some properties like name, roll number and has marks in some subjects.\nUsing matplotlib plot the following functions on the domain: x = 0.5 to 100.0 in steps of 0.5.\n\n\\(y = x\\)\n\\(y = x^2\\)\n\\(y = \\frac{x^3}{100}\\)\n\\(y = \\sin(x)\\)\n\\(y = \\frac{\\sin(x)}{x}\\)\n\\(y = \\log(x)\\)\n\\(y = e^x\\)\n\nUsing numpy generate a matrix of size 20X5 containing random numbers drawn uniformly from the range of 1 to 2. Using Pandas create a dataframe out of this matrix. Name the columns of the dataframe as “a”, “b”, “c”, “d”, “e”. Find the column with the highest standard deviation. Find the row with the lowest mean.\nAdd a new column to the dataframe called “f” which is the sum of the columns “a”, “b”, “c”, “d”, “e”. Create another column called “g”. The value in the column “g” should be “LT8” if the value in the column “f” is less than 8 and “GT8” otherwise. Find the number of rows in the dataframe where the value in the column “g” is “LT8”. Find the standard deviation of the column “f” for the rows where the value in the column “g” is “LT8” and “GT8” respectively.\nWrite a small piece of code to explain broadcasting in numpy.\nWrite a function to compute the argmin of a numpy array. The function should take a numpy array as input and return the index of the minimum element. You can use the np.argmin function to verify your solution."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "&lt;! – your comment ::: {.callout-tip} ## Summary\n\nGithub repo containing source code for lectures and notebooks\nPDF Lecture slides\nRendered Lecture notebooks\nYouTube playlist containing recodings\n\n:::\n\n\n\nLecture #\nDate\nTopic (Slides)\nNotebooks\nYouTube Recording\nReading\n\n\n\n\n0\n30 Dec 2023\nPre-requisite quiz released\n\n\n\n\n\n1\n3 Jan\nIntroduction and Logistics\nTranscription and local LLMs, Segmentation and Localization, Prompt to Image\nRecording\n\n\n\n2\n5 Jan\nConvention, Metrics, Classification, Regression\nRule based vs ML, Confusion Matrix, Dummy baselines\nRecording\n\n\n\n3\n8 Jan\nDecision Trees 1\nEntropy, Decision Trees Real Output, DT Real Input Discrete Output\nRecording\n\n\n\n4\n10 Jan\nDecision Trees 2, Bias and Variance\nBias variance, Decision Trees + Overfitting, Underfitting\nRecording\n\n\n\n5\n12 Jan\nTutorial on Numpy, Pandas, Classes, Cross-validation\nNumpy, Pandas, Entropy, Classes and Trees, Cross-validation and tuning hyperparameters\nTutorial recording, Lecture on Cross-validation\n\n\n\n6\n15 Jan\nCross-validation, Ensemble Methods\nCross-validation and tuning hyperparameters\nRecording\n\n\n\n7\n17 Jan\nLecture on Random Forests\n, Tutorial on Random Forests + Meshgrid\nVideo\nNotebook on Random Forests feature importance from scratch, Notebook on meshgrid\n\n\n8\n19 Jan\nLinear Regression\nRecording\n\n\n\n\n9\n24 Jan\nLinear Regression: Basis Functions\nRecording\nNotebook\n\n\n\n10\n25 Jan\nLinear Regression: Geometric Interpretation\nRecording\nNotebook\n\n\n\n10\n27 Jan\nMulti-colinearity, Dummy Variables\nRecording\nNotebook\n\n\n\n11\n29 Jan\nGradient Descent\nRecording\nNotebook on Taylor’s series, Notebook on Contour plots and gradient descent\n\n\n\n12\n31 Jan\nTutorial on GCloud + Forecasting + Polynomial Regression with Basis functions\n\nNotebook on Polynomial Regression, Notebook on Autoregressive Model\nArticle on Polynomial Regression\n\n\n13\n2 Feb\nQuiz 1\n\n\n\n\n\n14\n5 Feb\nGradient Descent\nRecording\nNotebook on Gradient Descent\n\n\n\n15\n7 Feb\nMovie recommendation, Matrix Factorisation\nRecording\nNotebook on Movie Recommendation\n\n\n\n16\n9 Feb\nRegularised Linear Regression: Ridge Regression, LASSO Regression\nRecording\n\n\n\n\n17\n12 Feb\nCoordinate Descent + Time Complexity + ML Maths + Convexity\nNot Recorded\nNotebook on Sklearn with GPU\n\n\n\n18\n14 Feb\nMock Quiz\n\n\n\n\n\n19\n16 Feb\nGuest Lecture by Arjun Bhagoji, Adversarial Examples for ML\n\nNotebook \n \n\n\n20\n26 Feb\nMid sem exam discussion\n\n\n\n\n\n21\n28 Feb\nLogistic Regression\nRecording\nNotebook on why use logits, Notebook on Logistic regression in Torch\n\n\n\n22\n1 March\nLogistic Regression\nRecording\nNotebook\n\n\n\n23\n5 March\nMLP I\nRecording\n\n\n\n\n24\n6 March\nMLP II\nRecording\nNotebook\n\n\n\n25\n7 March\nNext token prediction\nRecording\nNotebook\n\n\n\n26\n11 March\nMock Quiz\n\nSolution\n\n\n\n27\n13 March\nAutograd\nRecording\nNotebook\n\n\n\n28\n18 March\nCNN - I, 1D CNN\nRecording\nNotebook 1, Notebook 2 on edge detection, Notebook 3 on LeNet\n\n\n\n29\n20 March\nCNN - II\nRecording\nNotebook\n\n\n\n30\n22 March\nKNN\nRecording\nNotebook (Parametric v/s Non-Parametric), Notebook (Curse of dimensionality)\nReference on Parameteric v/s Non-Parametric from MLSS, ML Mastery, StackExchange, Sebastian Raschka’s blog\n\n\n31\n1 April\nReinforcement learning\nRecording\nNotebook 1, Notebook 2\n\n\n\n32\n3 April\nUnsupervised learning\nRecording\nNotebook\n\n\n\n33\n5 April\nApproximate KNN\nRecording\nNotebook\n\n\n\n34\n8 April\nConstrained Optimization 1, Constrained Optimization 2 (self study)\nPart 2 recording from older run\nNotebook\n\n\n\n35\n10 April\nSVM - I\nRecording\nNotebook 1\n\n\n\n36\n12 April\nSVM - II (Kernel)\nRecording\nNotebook, Notebook 2\n\n\n\n37\n15 April\nSVM - III (QP, Kernels as similarity)\nRecording\nNotebook on QP, Notebook on kernels-1\n\n\n\n38\n17 April\nSVM - IV (Kernel understanding, Soft Margin)\nRecording\nNotebook on kernel understanding, Notebook on soft margin\n\n\n\n39\n19 April\nOverall revision"
  }
]