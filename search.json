[
  {
    "objectID": "grading.html",
    "href": "grading.html",
    "title": "Grading Policy",
    "section": "",
    "text": "Quizzes: 48%\n\n16% each\nBest 3 out of 4\nIf you miss a single quiz for any reason, there will be no makeups\n\n\n\n\nAssignments: 44%\n\n4 or 5 assignments\nDone in groups of 5 6 (preferred) or 7\nAtleast one question per assignment will be a project-like question\nVariable weight (e.g. some assignments would be 10%, some 12%, etc.)\nSome assignments would involve:\n\nMaking pull requests to public repositories\nWriting Hugging Face Spaces like demos\n\n\n\n\n\nAttendance: 8%\n\n&lt;= 3 absences: 8%\n4 or 5 absences: 7%\n6 or 7 absences: 6%\n8 or 9 absences: 4%\n10 or 11 absences: 2%\n12 or more absences: 0%"
  },
  {
    "objectID": "tentative.html",
    "href": "tentative.html",
    "title": "Tentative deadlines",
    "section": "",
    "text": "January 24: First assignment due\nFebruary 2: First quiz\nEarly Feb: Second assignment released\nSometime in February 19-23 as per academic calendar during mid sem exams: Second quiz\nMarch 1: Second assignment due, third assignment released\nMarch 15: Third quiz\nMarch 21: Third assignment due, fourth assignment released\nApril 15: Fourth assignment due\nEnd sem: Fourth quiz"
  },
  {
    "objectID": "quizzes.html",
    "href": "quizzes.html",
    "title": "Quizzes",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nDec 30, 2023\n\n\nPrerequsite test\n\n\n \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "faq.html",
    "href": "faq.html",
    "title": "FAQ",
    "section": "",
    "text": "Projects\n\nIs there a project component in the course?\n\nYes and No. We will have a project-like question in each of your assignments. This will be a more “structured” project in that sense and all students would have the same question. However, there is no separate project component in the course. We feel this will help you get a better understanding of the concepts covered in the course, and the added structure will help you get started with the project.\n\nCan you give an example of the project question?\n\nSure. We will build an activity classifier: walking, running, etc. Initially, we use publicly available datasets to build the classifier. We will then help you with collecting your data using your smartphone and build a classifier using your data. Those who want to go beyond the learning objectives, can deploy the classifier on your smartphone.\n\n\n\n\n\nPrerequisites\n\nWhat are the course prerequisites?\n\nThe course has no “formal” prerequisites like all courses at IITGn. However, it is assumed that you have a basic understanding of:\n\nprogramming (Python), and data structures (ES242 equivalent)\nprobability and statistics\nlinear algebra\ncalculus\n\n\nWhat is the prerequisite exam?\n\nThis is an exam that we have designed to ensure that you have the necessary background to take the course.\n\nHow can I prepare for the prerequisite exam?\n\nYour UG course material should be sufficient to prepare for the exam.\nYou may additionally refer to the “prerequisite” reading section on the course homepage.\n\nWhat happens if I do not clear the prerequisite exam?\n\nYou will not be allowed to take the course.\n\nWhat do you mean by “clear” the exam?\n\nLike all courses at IITGn, the instructor will decide the cut-off for the exam. You will have to score above the cut-off to clear the exam. No cut-off will be revealed to the students apriori.\n\n\n\n\n\nLaptop policy\n\nCan I use my laptop in the class?\n\nNo. You are not allowed to use your laptop in the class for any reason.\n\n\n\n\nQuizzes\n\nWhat happens if I miss a quiz due to any reason?\n\nThe quiz will be marked as 0.\nThe provision of best 3 out of 4 quizzes is designed keeping in mind such scenarios.\n\nWill the quizzes and end-semester exam be open book? Will I be allowed to carry notes?\n\nNo, the exams and quizzes will be closed book. You are not permitted to carry notes.\n\nHow soon can I expect to receive my answer sheets back?\n\nYou should expect to receive answer sheets back in 4-5 working days.\n\nIs there an end-semester exam or mid-semester exam?\n\n2 out of the 4 quizzes will be held in the mid-semester and end-semester slots. The remaining two quizzes will be held during the semester.\n\nWill the quizzes be MCQs or subjective?\n\nThe quizzes may contain both the MCQs and subjective questions.\n\n\n\n\n\nAssignments\n\nWhat happens if I miss an assignment due to any reason?\n\nThere will no extensions for assignments.\n\nI have a doubt in the assignment. Whom should I write to?\n\nAsk on the slack #assignments channel. If you don’t get a response within 2 days, write to the course instructor.\n\nI do not know Python. Can I code assignments in some other language?\n\nUnfortunately, no. You have to stick to Python.\n\nHow will you evaluate the assignment?\n\nThe assignments would be followed by a viva. The TAs would first check the code and compare against the submission. Any change from the submitted code is not allowed and any instance of the same would culminate in a warning. The TAs would run the code and ask a few questions. About 75% of these questions would be based on the assignment in question and about 25% would be based on the theory behind the concepts covered in the assignment.\nThe grade breakup would be: i) code runs correctly and solves the problem [50% marks]; ii) questions based on the assignment and student understanding of code [25% marks]; iii) code quality [12.5% marks]; iv) questions based on the theory behind the concepts covered in the assignments [12.5% marks]\n\nIs the assignment individual or group?\n\nThe assignment is group. In case of group, all team members get the same grade for the assignment.\n\n\n\n\n\nAttendance\n\nAttendance policy\n\nAttendance is mentioned in the grading policy clearly.\n\n\n\n\n\nAudit policy\nYou are free to attend the lectures. However, given the limited number of TAs and the large number of students, we will not be able to grade your submissions. You can not formally contribute to any team in their assignments.\n\n\n\nComputational resources\nTypically, we have been kindly provided with Google Cloud credits. We will update this section once we have the credits. Once you get these credits, you can use them for your assignments. They are more than sufficient for the assignments."
  },
  {
    "objectID": "exams/prereq.html",
    "href": "exams/prereq.html",
    "title": "Prerequsite test",
    "section": "",
    "text": "Instructions\n\nThis test is open book, open internet, open notes. You can use any resources you want to solve the problems.\nYou should be typing your answers in a Jupyter notebook.\nThe submission would be a link to a public GitHub repository containing the notebook. Fill this form to submit your solution.\nA random subset of students may have a viva post the exam. The viva would be based on the notebook and the solutions you have provided.\nThe test is open till 6th January 2024 9 PM. You can submit your solutions anytime before that.\nThis problem has to be solved individually. You cannot collaborate with anyone else.\nThe code should be written using Python.\nSome questions may require you to answer in text. You can use markdown cells to write your answers. Some questions may require you to write code. You can use code cells to write your code. Some questions may require you to write mathematical expressions. You can use LaTeX to write your expressions. You can write such LaTeX expressions in markdown cells.\nFor any other questions, please ask on the General channel on Slack.\n\n\n\n\nQuestions\n\nHow many multiplications and additions do you need to perform a matrix multiplication between a (n, k) and (k, m) matrix? Explain.\nWrite Python code to multiply the above two matrices. Solve using list of lists and then use numpy. Compare the timing of both solutions. Which one is faster? Why?\nFinding the highest element in a list requires one pass of the array. Finding the second highest element requires 2 passes of the the array. Using this method, what is the time complexity of finding the median of the array? Can you suggest a better method? Can you implement both these methods in Python and compare against numpy.median routine in terms of time?\nWhat is the gradient of the following function with respect to x and y? \\[\nx^2y+y^3\\sin(x)\n\\]\nUse JAX to confirm the gradient evaluated by your method matches the analytical solution corresponding to a few random values of x and y\nUse sympy to confirm that you obtain the same gradient analytically.\nCreate a Python nested dictionary to represent hierarchical information. We want to store record of students and their marks. Something like:\n\n2022\n\nBranch 1\n\nRoll Number: 1, Name: N, Marks:\n\nMaths: 100, English: 70 …\n\n\nBranch 2\n\n2023\n\nBranch 1\nBranch 2\n\n2024\n\nBranch 1\nBranch 2\n\n2025\n\nBranch 1\nBranch 2\n\n\nStore the same information using Python classes. We have an overall database which is a list of year objects. Each year contains a list of branches. Each branch contains a list of students. Each student has some properties like name, roll number and has marks in some subjects.\nUsing matplotlib plot the following functions on the domain: x = 0.5 to 100.0 in steps of 0.5.\n\n\\(y = x\\)\n\\(y = x^2\\)\n\\(y = \\frac{x^3}{100}\\)\n\\(y = \\sin(x)\\)\n\\(y = \\frac{\\sin(x)}{x}\\)\n\\(y = \\log(x)\\)\n\\(y = e^x\\)\n\nUsing numpy generate a matrix of size 20X5 containing random numbers drawn uniformly from the range of 1 to 2. Using Pandas create a dataframe out of this matrix. Name the columns of the dataframe as “a”, “b”, “c”, “d”, “e”. Find the column with the highest standard deviation. Find the row with the lowest mean.\nAdd a new column to the dataframe called “f” which is the sum of the columns “a”, “b”, “c”, “d”, “e”. Create another column called “g”. The value in the column “g” should be “LT8” if the value in the column “f” is less than 8 and “GT8” otherwise. Find the number of rows in the dataframe where the value in the column “g” is “LT8”. Find the standard deviation of the column “f” for the rows where the value in the column “g” is “LT8” and “GT8” respectively.\nWrite a small piece of code to explain broadcasting in numpy.\nWrite a function to compute the argmin of a numpy array. The function should take a numpy array as input and return the index of the minimum element. You can use the np.argmin function to verify your solution."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Summary\n\n\n\n\nGithub repo containing source code for lectures and notebooks\nPDF Lecture slides\nRendered Lecture notebooks\nYouTube playlist containing recodings\n\n\n\n\n\n\nLecture #\nDate\nTopic (Slides)\nNotebooks\nYouTube Recording\nReading\n\n\n\n\n0\n30 Dec 2023\nPre-requisite quiz released\n\n\n\n\n\n1\n3 Jan\nIntroduction and Logistics\nTranscription and local LLMs, Segmentation and Localization, Prompt to Image\nRecording\n\n\n\n2\n5 Jan\nConvention, Metrics, Classification, Regression\nRule based vs ML, Confusion Matrix, Dummy baselines\nRecording\n\n\n\n3\n8 Jan\nDecision Trees 1\nEntropy, Decision Trees Real Output, DT Real Input Discrete Output\nRecording\n\n\n\n4\n10 Jan\nDecision Trees 2, Bias and Variance\nBias variance, Decision Trees + Overfitting, Underfitting\nRecording\n\n\n\n5\n12 Jan\nTutorial on Numpy, Pandas, Classes, Cross-validation\nNumpy, Pandas, Entropy, Classes and Trees, Cross-validation and tuning hyperparameters\nTutorial recording, Lecture on Cross-validation\n\n\n\n6\n15 Jan\nCross-validation, Ensemble Methods\nCross-validation and tuning hyperparameters\nRecording\n\n\n\n7\n17 Jan\nLecture on Random Forests\n, Tutorial on Random Forests + Meshgrid\nVideo\nNotebook on Random Forests feature importance from scratch, Notebook on meshgrid\n\n\n8\n19 Jan\nLinear Regression\nRecording\n\n\n\n\n9\n24 Jan\nLinear Regression: Basis Functions\nRecording\nNotebook\n\n\n\n10\n25 Jan\nLinear Regression: Geometric Interpretation\nRecording\nNotebook\n\n\n\n10\n27 Jan\nMulti-colinearity, Dummy Variables\nRecording\nNotebook\n\n\n\n11\n29 Jan\nGradient Descent\nRecording\nNotebook on Taylor’s series, Notebook on Contour plots and gradient descent\n\n\n\n12\n31 Jan\nTutorial on GCloud + Forecasting + Polynomial Regression with Basis functions\n\nNotebook on Polynomial Regression, Notebook on Autoregressive Model\nArticle on Polynomial Regression\n\n\n13\n2 Feb\nQuiz 1\n\n\n\n\n\n14\n5 Feb\nGradient Descent\nRecording\nNotebook on Gradient Descent\n\n\n\n15\n7 Feb\nMovie recommendation, Matrix Factorisation\nRecording\nNotebook on Movie Recommendation\n\n\n\n16\n9 Feb\nRegularised Linear Regression: Ridge Regression, LASSO Regression\nRecording\n\n\n\n\n17\n12 Feb\nCoordinate Descent + Time Complexity + ML Maths + Convexity\nNot Recorded\nNotebook on Sklearn with GPU\n\n\n\n18\n14 Feb\nMock Quiz\n\n\n\n\n\n19\n16 Feb\nGuest Lecture by Arjun Bhagoji, Adversarial Examples for ML\n\nNotebook \n \n\n\n20\n26 Feb\nMid sem exam discussion\n\n\n\n\n\n21\n28 Feb\nLogistic Regression\nRecording\nNotebook on why use logits, Notebook on Logistic regression in Torch\n\n\n\n22\n1 March\nLogistic Regression\nRecording\nNotebook\n\n\n\n23\n5 March\nMLP I\nRecording\n\n\n\n\n24\n6 March\nMLP II\nRecording\nNotebook\n\n\n\n25\n7 March\nNext token prediction\nRecording\nNotebook\n\n\n\n26\n11 March\nMock Quiz\n\nSolution\n\n\n\n27\n13 March\nAutograd\nRecording\nNotebook\n\n\n\n28\n18 March\nCNN - I, 1D CNN\nRecording\nNotebook 1, Notebook 2 on edge detection, Notebook 3 on LeNet\n\n\n\n29\n20 March\nCNN - II\nRecording\nNotebook\n\n\n\n30\n22 March\nKNN\nRecording\nNotebook (Parametric v/s Non-Parametric), Notebook (Curse of dimensionality)\nReference on Parameteric v/s Non-Parametric from MLSS, ML Mastery, StackExchange, Sebastian Raschka’s blog\n\n\n31\n1 April\nReinforcement learning\nRecording\nNotebook 1, Notebook 2\n\n\n\n32\n3 April\nUnsupervised learning\nRecording\nNotebook\n\n\n\n33\n5 April\nApproximate KNN\nRecording\nNotebook\n\n\n\n34\n8 April\nConstrained Optimization 1, Constrained Optimization 2 (self study)\nPart 2 recording from older run\nNotebook\n\n\n\n35\n10 April\nSVM - I\nRecording\nNotebook 1\n\n\n\n36\n12 April\nSVM - II (Kernel)\nRecording\nNotebook, Notebook 2\n\n\n\n37\n15 April\nSVM - III (QP, Kernels as similarity)\nRecording\nNotebook on QP, Notebook on kernels-1\n\n\n\n38\n17 April\nSVM - IV (Kernel understanding, Soft Margin)\nRecording\nNotebook on kernel understanding, Notebook on soft margin\n\n\n\n39\n19 April\nOverall revision"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ES 335 Machine Learning",
    "section": "",
    "text": "Please join the Slack channel for the course.\n\n\n\n\n\n\nSummary\n\n\n\n\nInstructor: Nipun Batra nipun.batra@iitgn.ac.in\n\nOffice: 13/401C\nYouTube video showing the directions to my office\n\nTeaching Assistants:\n\nRishiraj Adhikary rishiraj.a@iitgn.ac.in\nInderjeet Bhullar inderjeet.bhullar@iitgn.ac.in\nRahul Chembakasseril rahul.chembakasseril@iitgn.ac.in\nR Yeeshu Dhurandhar r.yeeshu@iitgn.ac.in\nSuraj Jaiswal jaiswalsuraj@iitgn.ac.in\nKalash Kankaria kalash.kankaria@iitgn.ac.in,\nSukruta Midigeshi sukruta.midigeshi@iitgn.ac.in\nChirag Sarda chirag.sarda@iitgn.ac.in,\nAyush Shrivastava shrivastavaayush@iitgn.ac.in\nSai Krishna avulasaikrishna@iitgn.ac.in\n\nCourse Timings:\n\nLectures: Monday, Wednesday 8:30 AM to 10 AM in 10/103\nTutorials: Friday: 1130 to 1 PM in 10/103\n\n\n\n\n\n\n\n\n\n\nPre-requisite exam\n\n\n\nThe pre-requisite exam has to be submitted by 6th January 2024, 9 PM. Details on how to submit the exam are given in the exam itself. The exam is open book and open internet.\n\n\n\n\nPre-requisites:\n\nGood experience in Python programming\nProbability\nLinear Algebra\n\nCourse preparation: Students are encouraged to study some of the following to refresh their understanding of some of the prerequisities before the course formally begins.\n\nFirst four chapters of the Python Data Science handbook\nSome material on Linear Algebra\nKhan academy course on Stats and Probability\n\n\n\nReference textbooks:\n\nGareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani. An Introduction to Statistical Learning with Applications in R\nChristopher Bishop. Pattern Recognition and Machine Learning. Springer, 2006.[Freely available online]\nFriedman J, Hastie T, Tibshirani R. The elements of statistical learning. New York, NY, USA:: Springer series in statistics; 2001.[Freely available online]\nDuda RO, Hart PE, Stork DG. Pattern classification. John Wiley & Sons; 2012 Nov 9.\nMitchell TM. Machine learning. 1997. Burr Ridge, IL: McGraw Hill. 1997;45(37):870-7.\nMurphy, K. Machine Learning: A Probabilistic Perspective. MIT Press\nGoodfellow I, Bengio Y, Courville A, Bengio Y. Deep learning. Cambridge: MIT press; 2016 Nov 18.[Freely available online]\n\n\n\nSome other ML courses\n\nNPTEL course by Balaram Ravindran\nCMU course by Tom Mitchell and Maria-Florina Balcan\nCoursera ML course by Andrew Ng\nFAST.ai course on ML\nPractical deep learning for coders by FAST.ai\nCourse by Alex Ihler, UCI"
  }
]